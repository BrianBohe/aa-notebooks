{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 1 \n",
    "### Clasificación sobre datos simulados. \n",
    "\n",
    "## Introducción\n",
    "Para este trabajo, hemos creado una función generadora de minions. Sobre cada minion, hemos medido 200 características que representan habilidades que poseen en distintas tareas (relacionadas al Mal).  \n",
    "\n",
    "El doctor Nefario ha ideado una fórmula para determinar si un minion es o no apto para concretar su plan para conquistar el mundo. De esta manera ha etiquetado más de 500 minions. Lamentablemente, ha perdido dicha fórmula y necesita seguir decidiendo si nuevos minions son o no aptos para su macabro plan.\n",
    "\n",
    "Es por esto que nuestro objetivo será construir clasificadores que estimen lo mejor posible la probabilidad de que nuevos minions sean o no aptos para concretar el plan de conquista y así facilitarle las cosas al doctor Nefario.\n",
    "\n",
    "Por otra parte, ya que el doctor Nefario tuvo problemas con equipos que sobreestiman sus resultados, decidió guardarse varias etiquetas extra que no compartirá con nadie, y que luego utilizará para elegir al mejor equipo, al cual contratará para (de una vez por todas) conquistar el mundo. \n",
    "\n",
    "\n",
    "En concreto:\n",
    "\n",
    "Tendrán disponible una matriz de datos $X$ de $500$ filas en donde cada fila $x^{(i)}$ representa un vector de $200$ características de cada instancia. Es decir, $\\textbf{x}^{(i)} = x_1^{(i)}, \\dots, x_{200}^{(i)}$ con $i$ entre $1$ y $500$. Además, tendrán y, un vector de $500$ posiciones con dos posibles valores: $True$ y $False$. \n",
    "\n",
    "Por otra parte, tendrán disponibles más instancias de evaluación $X_{competencia}$ sin las respectivas etiquetas que utilizaremos para evaluar sus resultados. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PREAMBULOS\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "# Commons imports\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from IPython.display import Image, display, HTML\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydotplus\n",
    "from time import time\n",
    "import itertools\n",
    "import graphviz\n",
    "\n",
    "from scipy.stats import randint as sp_randint, normaltest\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as  pd\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "## Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import sklearn.ensemble\n",
    "import sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common functions\n",
    "def dibujar_arbol(clf):\n",
    "    dot_data = StringIO()\n",
    "    sklearn.tree.export_graphviz(clf, out_file=dot_data,  \n",
    "                    filled=True,\n",
    "                    special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    display(Image(graph.create_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "X = pd.read_csv(\"X.csv\", index_col=\"index\")\n",
    "Y = pd.read_csv(\"y.csv\", index_col=\"index\", dtype=int)  # Cargamos los valores booleanos (True y False)\n",
    "                                                        # como números (1 y 0) para facilitar el manejo luego. \n",
    "\n",
    "X_competencia = pd.read_csv(\"X_competencia.csv\", index_col=\"index\")\n",
    "Y_competencia_ejemplo = pd.read_csv(\"y_competencia_ejemplo.csv\", index_col=\"index\")\n",
    "# Descomentar si quieren ver los datos para la competencia:\n",
    "# display(X_competencia) \n",
    "# display(y_competencia_ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descripción del DataFrame con (muestras x predictores)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0715</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>-0.0103</td>\n",
       "      <td>-0.0436</td>\n",
       "      <td>-0.0208</td>\n",
       "      <td>-0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0187</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>-0.0356</td>\n",
       "      <td>-0.1940</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>-0.0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.0153</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>1.0360</td>\n",
       "      <td>1.0230</td>\n",
       "      <td>1.0647</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9731</td>\n",
       "      <td>0.9716</td>\n",
       "      <td>1.0075</td>\n",
       "      <td>1.0246</td>\n",
       "      <td>0.9934</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.1722</td>\n",
       "      <td>-2.4596</td>\n",
       "      <td>-2.8834</td>\n",
       "      <td>-3.7474</td>\n",
       "      <td>-2.9987</td>\n",
       "      <td>-3.2014</td>\n",
       "      <td>-3.6855</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.9110</td>\n",
       "      <td>-2.9642</td>\n",
       "      <td>-2.5163</td>\n",
       "      <td>-3.9278</td>\n",
       "      <td>-2.4254</td>\n",
       "      <td>-2.6234</td>\n",
       "      <td>-2.8690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.6090</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>-0.7180</td>\n",
       "      <td>-0.6594</td>\n",
       "      <td>-0.7177</td>\n",
       "      <td>-0.6510</td>\n",
       "      <td>-0.7073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-0.6509</td>\n",
       "      <td>-0.6613</td>\n",
       "      <td>-0.8689</td>\n",
       "      <td>-0.6466</td>\n",
       "      <td>-0.6354</td>\n",
       "      <td>-0.6855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>-0.0713</td>\n",
       "      <td>0.0612</td>\n",
       "      <td>-0.0097</td>\n",
       "      <td>-0.0407</td>\n",
       "      <td>-0.0771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0473</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>-0.0749</td>\n",
       "      <td>-0.1901</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>-0.0332</td>\n",
       "      <td>-0.0797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.6334</td>\n",
       "      <td>0.7670</td>\n",
       "      <td>0.7066</td>\n",
       "      <td>0.6699</td>\n",
       "      <td>0.6616</td>\n",
       "      <td>0.6508</td>\n",
       "      <td>0.6029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>0.6860</td>\n",
       "      <td>0.5743</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.6608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.9702</td>\n",
       "      <td>2.7920</td>\n",
       "      <td>2.6905</td>\n",
       "      <td>2.8091</td>\n",
       "      <td>2.9823</td>\n",
       "      <td>2.9342</td>\n",
       "      <td>3.3240</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0983</td>\n",
       "      <td>3.1469</td>\n",
       "      <td>2.9109</td>\n",
       "      <td>2.4942</td>\n",
       "      <td>3.1804</td>\n",
       "      <td>3.0034</td>\n",
       "      <td>2.5107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "count  500.0000  500.0000  500.0000  500.0000  500.0000  500.0000  500.0000   \n",
       "mean     0.0384    0.0715    0.0056   -0.0103   -0.0436   -0.0208   -0.0571   \n",
       "std      1.0153    0.9613    1.0360    1.0230    1.0647    0.9898    1.0000   \n",
       "min     -3.1722   -2.4596   -2.8834   -3.7474   -2.9987   -3.2014   -3.6855   \n",
       "25%     -0.6090   -0.6207   -0.7180   -0.6594   -0.7177   -0.6510   -0.7073   \n",
       "50%      0.0602    0.0560   -0.0713    0.0612   -0.0097   -0.0407   -0.0771   \n",
       "75%      0.6334    0.7670    0.7066    0.6699    0.6616    0.6508    0.6029   \n",
       "max      2.9702    2.7920    2.6905    2.8091    2.9823    2.9342    3.3240   \n",
       "\n",
       "         ...          193       194       195       196       197       198  \\\n",
       "count    ...     500.0000  500.0000  500.0000  500.0000  500.0000  500.0000   \n",
       "mean     ...      -0.0187    0.0087   -0.0356   -0.1940    0.0250    0.0257   \n",
       "std      ...       0.9731    0.9716    1.0075    1.0246    0.9934    0.9940   \n",
       "min      ...      -2.9110   -2.9642   -2.5163   -3.9278   -2.4254   -2.6234   \n",
       "25%      ...      -0.6441   -0.6509   -0.6613   -0.8689   -0.6466   -0.6354   \n",
       "50%      ...      -0.0473    0.0537   -0.0749   -0.1901    0.0185   -0.0332   \n",
       "75%      ...       0.6071    0.6860    0.5743    0.4636    0.7041    0.6575   \n",
       "max      ...       3.0983    3.1469    2.9109    2.4942    3.1804    3.0034   \n",
       "\n",
       "            199  \n",
       "count  500.0000  \n",
       "mean    -0.0036  \n",
       "std      0.9819  \n",
       "min     -2.8690  \n",
       "25%     -0.6855  \n",
       "50%     -0.0797  \n",
       "75%      0.6608  \n",
       "max      2.5107  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Descripción del DataFrame con (muestras x predictores)')\n",
    "display(X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observación:\n",
    "A primera vista parece que los predictores tienen una distribución gaussiana con media $\\approx 0$ y varianza $\\approx 1$. Se puede realizar un test de normalidad aunque esto sólo puede permitirnos rechazar la hipótesis nula: \"los datos provienen de una distribución normal con media $\\mu$ y varianza $\\sigma^2$\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se puede rechazar la hipótesis de que provengan de una normal para las features:\n",
      " [2, 3, 6, 10, 18, 33, 46, 83, 94, 97, 106, 108, 119, 128, 138, 142, 157, 160, 165, 168, 179, 191]\n"
     ]
    }
   ],
   "source": [
    "def test_normality_on(values, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Computes for each features a normality test.\n",
    "    \n",
    "    This function tests the null hypothesis (or H0) that a sample comes\n",
    "    from a normal distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    values : array_like\n",
    "        Matrix of shape (samples x features) containing the values of the features for each sample.\n",
    "    \n",
    "    pvalue : float\n",
    "        A threshold to reject or not H0.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    h0_is_rejected : array-like\n",
    "        A bit-mask which has each index has value 1 if H0 is rejected for the given alpha value,\n",
    "        0 in other case.\n",
    "    \"\"\"\n",
    "    # we use two different tests\n",
    "    _statistic, _p_values = normaltest(values)\n",
    "    # _statistic should not live in [0,4] to reject H0. _k2 is non-negative array.\n",
    "    # _p_values should be lower than alpha to reject H0. _k2 is an array.\n",
    "    \n",
    "    # rejects if at least one of both rejects\n",
    "    return np.array(\n",
    "        [\n",
    "            _p_values[_ith_feature] < alpha or 4 < _statistic[_ith_feature] \n",
    "             for _ith_feature in range(len(_p_values))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "normality_is_rejected = test_normality_on(X)\n",
    "\n",
    "features_which_reject_h0 = []\n",
    "for index, is_rejected in enumerate(normality_is_rejected):\n",
    "    if is_rejected:\n",
    "        features_which_reject_h0.append(index)\n",
    "\n",
    "print('Se puede rechazar la hipótesis de que provengan de una normal para las features:\\n {}'.format(\n",
    "    features_which_reject_h0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Hay correlación entre variables?\n",
    "\n",
    "Si dos variables son independientes, entonces su correlación es 0. El contrarrecíproco nos permite mostrar que al tener una correlación distinta de 0, las variables no son independientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuál es el porcentaje de cada clase en la muestra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.4580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.4987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         output\n",
       "count  500.0000\n",
       "mean     0.4580\n",
       "std      0.4987\n",
       "min      0.0000\n",
       "25%      0.0000\n",
       "50%      0.0000\n",
       "75%      1.0000\n",
       "max      1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Clase 1: 0.46\n",
      "% Clase 0: 0.54\n"
     ]
    }
   ],
   "source": [
    "display(Y.describe())\n",
    "\n",
    "class_1_percentage = int(Y.sum()) / len(Y)\n",
    "\n",
    "print('% Clase 1: {:.2f}\\n% Clase 0: {:.2f}'.format(class_1_percentage, 1 - class_1_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "### Separación de datos\n",
    "\n",
    "Contarán con una cantidad limitada de datos, por lo cual es importante tomar una buena decisión en el momento de empezar a utilizarlos. En este punto pedimos que evalúen cómo separar sus datos para desarrollo y para evaluación tomando en cuenta la competencia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percentage_train = 9/10\n",
    "X_train, X_held_out, Y_train, Y_held_out = sklearn.model_selection.train_test_split(\n",
    "    X, Y, train_size=percentage_train, stratify=Y\n",
    ")\n",
    "\n",
    "# Just to check sizes and percentage of class 1 in each one\n",
    "print(\"Tamaño conj de entrenamiento: {}, tamaño conj de clases de entrenamiento: {}. porcentaje de clase 1 en conj {:.2f}.\".format(\n",
    "        X_train.shape, Y_train.shape, int(Y_train.sum())/len(Y_train))\n",
    ")\n",
    "print(\"Tamaño conj de test: {}, tamaño conj de clases de test: {}, porcentaje de clase 1 en conj {:.2f}.\".format(\n",
    "        X_held_out.shape, Y_held_out.shape, int(Y_held_out.sum())/len(Y_held_out))\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(np.array(Y_held_out))  # muestra un histograma para la distribución de y.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "### Construcción de modelos\n",
    "\n",
    "Para este punto, la tarea consiste en construir y evaluar modelos de tipo árbol de decisión, de manera de obtener una estimación realista de la performance de los mismos. \n",
    "\n",
    "1. Entrenar un árbol de decisión con altura máxima 3 y el resto de los hiperparámetros en default. \n",
    "2. Estimar la performance del modelo utilizando K-fold cross validation con K = 5, con las métricas “Accuracy” y “ROC AUC”. Para ello, se pide medir la performance en cada partición tanto sobre el fold de validación como sobre los folds de entrenamiento. Luego, completar la primera tabla.\n",
    "3. Entrenar árboles de decisión para cada una de las siguientes combinaciones y completar la segunda tabla.\n",
    "\n",
    "----\n",
    "\n",
    "**EJERCICIO EXTRA: Usar la implementación de árboles de decisión que realizaron para la guía de ejercicios de la materia. Adaptarla para que cumpla con la interfaz requerida por sklearn, asegurarse de que funcione con variables continuas y reproducir las tablas anteriores.   **\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_accuracy_and_auc_roc_on_train_and_test(metrics_results,\n",
    "        tabble_header='Resultado Ejercicio 2.2: accuracy y roc auc en árboles de decisión con altura máxima de 3'):\n",
    "    \"\"\"\n",
    "    Param metrics_results :type: dict should at least contains keys \n",
    "    ['train_accuracy', 'test_accuracy', 'train_roc_auc', 'test_roc_auc'].\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(index=range(1,6))\n",
    "    df.index.name = \"Fold de validación\"\n",
    "\n",
    "    df[\"Accuracy (entrenamiento)\"] = metrics_results['train_accuracy']\n",
    "    df[\"Accuracy (validación)\"] = metrics_results['test_accuracy']\n",
    "    df[\"AUC ROC (entrenamiento)\"] = metrics_results['train_roc_auc']\n",
    "    df[\"AUC ROC (validación)\"] = metrics_results['test_roc_auc']\n",
    "    display(HTML('<h3> {} </h3>'.format(tabble_header)))\n",
    "    display(df)\n",
    "\n",
    "    fig = df.plot(kind=\"bar\")\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "X_train_np = np.array(X_train)\n",
    "Y_train_np = np.array(Y_train).ravel()\n",
    "\n",
    "\n",
    "# In order to avoid using the same variable's names in different excercises,\n",
    "# each excercise is a function with local variables\n",
    "\n",
    "def excercise_2_2():\n",
    "    decision_tree = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "    metric_scores = sklearn.model_selection.cross_validate(\n",
    "        decision_tree, X_train_np, Y_train_np, scoring=('roc_auc', 'accuracy'), cv=5, n_jobs=4\n",
    "    )\n",
    "\n",
    "    plot_accuracy_and_auc_roc_on_train_and_test(metric_scores)\n",
    "    return\n",
    "\n",
    "excercise_2_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_table_excercise_2_3(metrics_scores,\n",
    "                             table_header='Resultado Ejercicio 2.3: árboles de decisión con diferentes hiperparámetros'):\n",
    "    \"\"\"\n",
    "    Param metrics_scores :type: dict should at least has 'train_results' and 'test_results' keys\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(index=range(0,6))\n",
    "\n",
    "    df['Altura Máxima'] = [3, 5, 'Inifinity'] * 2\n",
    "    df['Medida de Separación'] = ['Gini'] * 3 + ['Ganancia de Información'] * 3\n",
    "    df['Promedio AUC ROC (entrenamiento)'] = metrics_scores['train_results']\n",
    "    df['Promedio AUC ROC (validación)'] = metrics_scores['test_results']\n",
    "   \n",
    "    display(HTML(\"<h3> {} </h3>\".format(table_header)))\n",
    "    display(df)\n",
    "    \n",
    "    df.plot(kind=\"bar\")\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "def excercise_2_3():\n",
    "    metric_scores = { 'train_results' : [], 'test_results' : [] }\n",
    "            \n",
    "    for _criterio, _depth in itertools.product(['gini', 'entropy'], [3, 5, None]):\n",
    "        \n",
    "        decision_tree = DecisionTreeClassifier(max_depth=_depth, criterion=_criterio)\n",
    "        \n",
    "        tree_metric_score = sklearn.model_selection.cross_validate(\n",
    "            decision_tree, X_train_np, Y_train_np, scoring='roc_auc', cv=5\n",
    "        )\n",
    "            \n",
    "        metric_scores['train_results'].append(np.mean(tree_metric_score['train_score']))\n",
    "        metric_scores['test_results'].append(np.mean(tree_metric_score['test_score']))\n",
    "    \n",
    "    plot_table_excercise_2_3(metric_scores)\n",
    "        \n",
    "excercise_2_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Comparación de algoritmos\n",
    "\n",
    "\n",
    "Se pide explorar distintas combinaciones de algoritmos de aprendizaje e hiperparámetros, de manera de buscar una performance óptima. Para este ejercicio es necesario que evalúen posibilidades utilizando la técnica de Grid Search. Como métrica de performance, usar siempre el área bajo la curva (AUC ROC) resultante de 5-fold cross-validation. \n",
    "\n",
    "Algoritmos a probar: KNN, árboles de decisión, LDA, Naive Bayes y SVM. Hiperparámetros: Revisar la documentación de cada uno para la búsqueda de combinaciones prometedoras.  \n",
    "\n",
    "Se pide generar un reporte que contenga: \n",
    "\n",
    "1. Una descripción de las distintas combinaciones consideradas y su performance asociada (las que consideren relevantes, con al menos la mejor combinación para cada algoritmo). \n",
    "\n",
    "1. Una breve explicación de los factores que creen que produjeron dicho resultado. \n",
    "\n",
    "En este punto evaluaremos tanto los hiperparámetros elegidos como las conclusiones relacionadas a por qué piensan que ciertos algoritmos funcionan mejor que otros para estos datos. \n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**EJERCICIO EXTRA**: Utilizar RandomizedSearchCV con rangos de parámetros que contengan a los utilizados en el GridSearch. Analizar si se encontraron mejores combinaciones de parámetros que no hayan sido tenidas en cuenta con el GridSearch y cuál fue la diferencia de tiempo de ejecución. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_results(grid, top=5, skip=0):\n",
    "    print(\"Top {} Salteando {} combinaciones\".format(top, skip))\n",
    "    df = pd.DataFrame(grid.cv_results_[\"params\"])\n",
    "    df[\"mean_score_validation\"] = grid.cv_results_[\"mean_test_score\"]\n",
    "    df[\"mean_score_training\"] = grid.cv_results_[\"mean_train_score\"]\n",
    "    display(df.sort_values(by=\"mean_score_validation\", ascending=False).head(top+skip).tail(top))\n",
    "    return\n",
    "\n",
    "def apply_grid_search_on_classifier_and_parameters(\n",
    "    classifier, classifier_parameters, _X_train, _Y_train, metrics_names=('roc_auc'), cv=5, \n",
    "    n_jobs=4, error_score='raise'):\n",
    "    \n",
    "    _clf = GridSearchCV(\n",
    "        classifier, classifier_parameters, scoring=metrics_names, cv=cv, n_jobs=n_jobs,\n",
    "        error_score=error_score\n",
    "    )\n",
    "    \n",
    "    return _clf.fit(_X_train, _Y_train)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search sobre clasificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to apply grid on each Classifier with its own possible arguments  \n",
    "\n",
    "def knn_grid_search_parametrization(_X_train, _Y_train):\n",
    "    knn_arguments = [\n",
    "        {\n",
    "            'algorithm': ['brute'],\n",
    "            'n_neighbors': range(1, 350, 50),\n",
    "            'weights': ['distance','uniform']\n",
    "        },\n",
    "        {\n",
    "            'algorithm': ['ball_tree', 'kd_tree'],\n",
    "            'n_neighbors': range(1, 350, 50),\n",
    "            'weights': ['distance','uniform'],\n",
    "            'p': [1, 2, 200],\n",
    "            'leaf_size': range(30, 150, 30)\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    \n",
    "    return apply_grid_search_on_classifier_and_parameters(knn_classifier, knn_arguments, _X_train, _Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tree_grid_search_parametrization(_X_train, _Y_train, _error_score='raise'):\n",
    "    \n",
    "    _amount_of_features_in_dataset = _X_train.shape[1]\n",
    "    _possible_max_features_ = list(range(25, min(200, _amount_of_features_in_dataset), 10)) \\\n",
    "        + list(filter(lambda x: x is None or x <= _amount_of_features_in_dataset, [7, 14, 20, None]))\n",
    "    tree_arguments = [\n",
    "        {\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': list(range(3, 13, 3)) + list(range(15, 36, 10)) + [None],\n",
    "            'max_features': _possible_max_features_,\n",
    "            'max_leaf_nodes': list(range(10, 501, 50)) + [None]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    tree_classifier = DecisionTreeClassifier(random_state = SEED) # it doesn't seem to be taking numpy.seed\n",
    "    \n",
    "    return apply_grid_search_on_classifier_and_parameters(\n",
    "        tree_classifier, tree_arguments, _X_train, _Y_train, error_score=_error_score\n",
    "    )\n",
    "\n",
    "#grid_tree = tree_grid_search_parametrization(X_train_np, Y_train_np)\n",
    "top_results(grid_tree,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros\n",
    "Se varía la forma de medir la calidad de una separación (criterion) en un nodo como también la forma en la que se elige entre las posibles (splitter). Se varía también la cantidad máxima de predictores a considerar para buscar la mejor separación. \n",
    "Nos pareció de interés variar la altura y la cantidad de hojas permitidas en el árbol para ver como generaliza una árbol pequeño o con pocas hojas respecto a uno sin limitación.\n",
    "\n",
    "### Resultados\n",
    "Se obtuvieron mejores resultados en los datos de validación respecto a los obtenidos en el ejercicio 2.2 y 2.3 con resultados en los datos de entrenamiento menores (el top 10 está por debajo de %90).\n",
    "\n",
    "Si nuestros datos hubieran sido linealmente separables, hubieramos obtenido mejores resultados.\n",
    "    \n",
    "    \n",
    "Los predictores son permutados en cada nodo al momento de buscar una separación, por lo que considerar diferentes de no fijar una semilla, mismos parámetros y datos pueden concluir en árboles distintos. Se fijo el parámetro $\\textit{random_state}$ con el mismo valor que la seed de numpy para obtener de manera determinista un árbol.\n",
    "\n",
    "Algunas preguntas que podemos hacernos son\n",
    "\n",
    "1. ¿Qué features separa y cómo el mejor árbol obtenido?\n",
    "2. ¿Cómo afecta el cambiar de algunos hiperaparámetros?\n",
    "    + ¿Cómo cambia el auc roc mientras aumentamos la profundidad máxima del árbol?\n",
    "    + ¿Cómo cambia el auc roc mientras aumentamos la cantidad de hojas máxima del árbol?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)¿Qué features separa y cómo el mejor árbol obtenido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Functions for DecisionTreeClassifier and its internal Tree struct\n",
    "\n",
    "def create_decision_tree_classifier_with_params(parameter_combination):\n",
    "    criterion = parameter_combination['criterion']\n",
    "    max_depth = parameter_combination['max_depth']\n",
    "    max_features = parameter_combination['max_features']\n",
    "    max_leaf_nodes = parameter_combination['max_leaf_nodes']\n",
    "    splitter = parameter_combination['splitter']\n",
    "    \n",
    "    return DecisionTreeClassifier(\n",
    "            criterion=parameter_combination['criterion'],\n",
    "            max_depth= None if max_depth is None or np.isnan(max_depth) else int(max_depth),\n",
    "            max_features= None if max_features is None or np.isnan(max_features) else int(max_features),\n",
    "            max_leaf_nodes= None if max_leaf_nodes is None or np.isnan(max_leaf_nodes) else int(max_leaf_nodes),\n",
    "            splitter=parameter_combination['splitter'],\n",
    "            random_state=SEED\n",
    "    )\n",
    "\n",
    "def calculate_nodes_depth_and_leaves(tree_classifier):\n",
    "    \"\"\"\n",
    "    Param tree_classifier :type: DecisionTreeClassifier.\n",
    "    \n",
    "    Traverse the internal tree_ structure tree_classifier in order to compute, its depth, amount of nodes\n",
    "    and amount of leaves.\n",
    "    \n",
    "    Trivial tree (only one node) has depth equals to 0.\n",
    "    \"\"\"\n",
    "    n_nodes = tree_classifier.tree_.node_count\n",
    "    children_left = tree_classifier.tree_.children_left\n",
    "    children_right = tree_classifier.tree_.children_right\n",
    "\n",
    "    # The tree structure can be traversed to compute various properties such\n",
    "    # as the depth of each node and whether or not it is a leaf.\n",
    "    max_depth_used = -1\n",
    "    leaves_amount = 0\n",
    "    stack = [(0, max_depth_used)]  # (root_node_id, its_parent_depth)\n",
    "    while len(stack) > 0:\n",
    "        node_id, parent_depth = stack.pop()\n",
    "        \n",
    "        max_depth_used = max(max_depth_used, parent_depth+1) # keep max depth\n",
    "        \n",
    "        # If it's a leaf its childs are equal\n",
    "        if (children_left[node_id] != children_right[node_id]):\n",
    "            stack.append((children_left[node_id], parent_depth + 1))\n",
    "            stack.append((children_right[node_id], parent_depth + 1))\n",
    "        else:\n",
    "            leaves_amount += 1\n",
    "    \n",
    "    return n_nodes, max_depth_used, leaves_amount # nodes_count, max_depth_used, leaves_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_first(grid, top=10):\n",
    "    df = pd.DataFrame(grid.cv_results_[\"params\"])\n",
    "    df[\"mean_score_validation\"] = grid.cv_results_[\"mean_test_score\"]\n",
    "    df[\"mean_score_training\"] = grid.cv_results_[\"mean_train_score\"]\n",
    "    df = df.sort_values(by=\"mean_score_validation\", ascending=False).head(top)\n",
    "    \n",
    "    for _, _parameters in df.iterrows():\n",
    "        print(_parameters)\n",
    "        best_tree = create_decision_tree_classifier_with_params(_parameters)\n",
    "        best_tree.fit(X_train_np, Y_train_np)\n",
    "        print('Cantidad de nodos {}\\nAltura del árbol {}\\nCantidad de hojas {}'.format(\n",
    "                *calculate_nodes_depth_and_leaves(best_tree))\n",
    "        )\n",
    "        dibujar_arbol(best_tree)\n",
    "    return\n",
    "\n",
    "# plot first 10 trees to see their shape and the features they used to split in each node \n",
    "print_first(grid_tree,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observación:\n",
    "En el top 10 vemos presentes muchos arboles con criterio $\\textit{entropy}$ con el mismo valor de AUC ROC. Esto es sólo ocurre porque encontramos el mismo árbol con una altura final de 3 repetido para cada $\\textit{max_depth}$. No se expande más debido al threshold de mínima reducción de impuereza (default = 0.0).\n",
    "\n",
    "Si evitamos estos resultados con misma altura, podremos ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) ¿Cómo afecta el cambiar de algunos hiperaparámetros?\n",
    "    - ¿Cómo cambia el auc roc mientras aumentamos la profundidad máxima del árbol?\n",
    "    - ¿Cómo cambia el auc roc mientras aumentamos la cantidad de hojas máxima del árbol?\n",
    "    \n",
    "\n",
    "Para ver la primera pregunta consideraramos los árboles sin límite en la cantidad de hojas y en la segunda los árboles sin límite en la cantidad profundidad. Creamos un árbol para cada combinación de hiperparámetros utilizando la misma semilla pero en este caso fiteamos sobre todo el set de datos de entrenamiento y no sobre un fold sabiendo que el árbol no será exactamente el mismo pero 'similar' (el valor que nos muestra GridSearch es un promedio de los obtenidos por los folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_nodes_depth_and_leaves_features_of_tree_for_each_parameter_combination(\n",
    "    parameter_combination_df, _X_train, _Y_train):\n",
    "    \"\"\"\n",
    "    Param parameter_combination_df :type: DataFrame contains a serie of hiperparameters to create\n",
    "    DecisionTreeClassifiers.\n",
    "    \n",
    "    For each one compute the depth and amount of leaves and nodes of the internval tree structure.\n",
    "    This is information is returned in the input DataFrame as new columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    amount_nodes = np.zeros(shape=len(parameter_combination_df.index), dtype=np.int64)\n",
    "    depth_used = np.zeros(shape=len(parameter_combination_df.index), dtype=np.int64)\n",
    "    amount_leaves = np.zeros(shape=len(parameter_combination_df.index), dtype=np.int64)\n",
    "    \n",
    "    _index=0\n",
    "    for _, parameter_combination in parameter_combination_df.iterrows():\n",
    "        _tree_classifier = create_decision_tree_classifier_with_params(parameter_combination)\n",
    "        _tree_classifier.fit(X_train, Y_train)\n",
    "        amount_nodes[_index], depth_used[_index], amount_leaves[_index] = calculate_nodes_depth_and_leaves(_tree_classifier)\n",
    "        _index +=1\n",
    "    \n",
    "    parameter_combination_df['amount_nodes'] = amount_nodes\n",
    "    parameter_combination_df['depth_used'] = depth_used\n",
    "    parameter_combination_df['amount_leaves'] = amount_leaves\n",
    "    \n",
    "    return parameter_combination_df\n",
    "\n",
    "def plot_dataframe_as_bar(dataframe):\n",
    "    fig = dataframe.plot(kind=\"bar\", figsize=(18.5, 10.5))\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def plot_score_while_increasing_depth(df_without_depth_bound, _X_train, _Y_train):\n",
    "    df_without_depth_bound = add_nodes_depth_and_leaves_features_of_tree_for_each_parameter_combination(\n",
    "        df_without_depth_bound, _X_train, _Y_train\n",
    "    )\n",
    "    \n",
    "    df_without_depth_bound = df_without_depth_bound.drop(\n",
    "        columns=['max_depth']\n",
    "    )[['depth_used','mean_score_validation', 'mean_score_training']].groupby(['depth_used']).mean()\n",
    "    \n",
    "    plot_dataframe_as_bar(df_without_depth_bound)\n",
    "    return\n",
    "\n",
    "def plot_score_while_increasing_leaves(df_without_leaves_bound, _X_train, _Y_train):\n",
    "    df_without_leaves_bound = add_nodes_depth_and_leaves_features_of_tree_for_each_parameter_combination(\n",
    "        df_without_leaves_bound, _X_train, _Y_train\n",
    "    )\n",
    "    \n",
    "    df_without_leaves_bound = df_without_leaves_bound.drop(\n",
    "        columns=['max_leaf_nodes']\n",
    "    )[['amount_leaves','mean_score_validation', 'mean_score_training']].groupby(['amount_leaves']).mean()\n",
    "    \n",
    "    plot_dataframe_as_bar(df_without_leaves_bound)\n",
    "    return\n",
    "    \n",
    "def plot_score_while_increasing_depth_and_max_feature(grid, top=2):\n",
    "    df = pd.DataFrame(grid.cv_results_[\"params\"])\n",
    "    df[\"mean_score_validation\"] = grid.cv_results_[\"mean_test_score\"]\n",
    "    df[\"mean_score_training\"] = grid.cv_results_[\"mean_train_score\"]\n",
    "    \n",
    "    df_without_depth_bound = df[np.isnan(df['max_depth'])].head(top)\n",
    "    plot_score_while_increasing_depth(df_without_depth_bound, X_train_np, Y_train_np)\n",
    "    \n",
    "    df_without_leaves_bound = df[np.isnan(df['max_leaf_nodes'])].head(top)\n",
    "    plot_score_while_increasing_leaves(df_without_leaves_bound, X_train_np, Y_train_np)\n",
    "    return\n",
    "\n",
    "plot_score_while_increasing_depth_and_max_feature(grid_tree, len(grid_tree.cv_results_[\"mean_test_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que aumentar el límite de hojas y altura del árbol aumenta en promedio el valor de la métrica auc roc a %100 (aproximadamente) sobre los datos de entrenamiento de manera rápida (cantidad de hojas mayor a 30 o profundidad mayor a 8) mientras que la misma métrica sobre los datos de validación empeoran.\n",
    "\n",
    "Podemos justificar esto en el overfitting de los árboles al no limitar el tamaño y de los mismos. \"Árboles más pequeños, generalizan mejor\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_bayes_grid_search_parametrization(_X_train, _Y_train):\n",
    "    \"\"\"\n",
    "    The only parameter of Naive-Bayes classifier are the prior probabilities of each class,\n",
    "    which are automatically calculated as #items_class_1 / #items and #items_class_2 / #items\n",
    "    if no parameter is specified.\n",
    "    \n",
    "    I doesn't make sense trying other prior probabilities than the real ones if the data isn't changing.\n",
    "    \"\"\"\n",
    "    naive_bayes_classifier = GaussianNB()\n",
    "    naive_arguments = [{}] #None\n",
    "    \n",
    "    return apply_grid_search_on_classifier_and_parameters(\n",
    "        naive_bayes_classifier, naive_arguments, _X_train, _Y_train\n",
    "    )\n",
    "\n",
    "#grid_naive = naive_bayes_grid_search_parametrization(X_train_np, Y_train_np, metric_name='roc_auc')\n",
    "top_results(grid_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros\n",
    "\n",
    "Los parámetros de este clasificador son las probabilidades a priori de cada clase. Esta es información que no conocemos pero que podríamos estimar utilizando la frecuencia relativa de nuestra muestra de entrenamiento. Esto mismo es lo que hace el clasificador si ningún parámetro es especificado.\n",
    "\n",
    "### Resultados\n",
    "\n",
    "Los resultados son mejores que los obtenidos utilizando árboles de decisión.\n",
    "\n",
    "Este método tiene dos asunciones:\n",
    "    1. Los predictores son independientes entre sí (propio del método)\n",
    "    2. Las probabilidad condicional P(X_i=x_i/Y=k) de observar el valor x_i para el predictor X_i dado que la muestra x pertenece a la clase k se comporta como una gaussiana (propio de la forma en que está implementado en python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Para responder la primera asunción basta observar la matriz de covarianza de los predictores en el análisis del input. Si dos variables son independientes su covarianza es 0. No vale la vuelta pero podemos usar el contrarrecíproco: si la covarianza entre dos predictores es distinta 0 entonces no son independientes.\n",
    "\n",
    "#### Observación: podría ser que al splitear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_correlation_between_dataframe_columns(df_features):\n",
    "    corr = df_features.corr()\n",
    "    fig = plt.figure(figsize=(10, 10), dpi=200)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    fig.colorbar(cax)\n",
    "    ticks = list(map(int, np.linspace(0,len(X_train.columns)-1,20)))\n",
    "    ax.set_xticks(ticks)\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_xticklabels(df_features.columns[ticks])\n",
    "    ax.set_yticklabels(df_features.columns[ticks])\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "plot_correlation_between_dataframe_columns(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico anterior nos muestra que hay correlación lineal mayor a 0.5 entre varios predictores, y con esto no independencia entre los mismos. No podemos decir nada respecto a aquellos predictores $P_1$, $P_2$, con $|\\text{Covarianza}(P_1,P_2)| < 0 + \\textit{epsilon}$\n",
    "\n",
    "donde $\\textit{epsilon}$ es algún valor que admita un error (ej: [0, 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Para observar si era un error asumir que podían aproximarse las probabilidades condicionales con gaussianas se puede hacer test de normalidad, pero ahora sobre los datos que pertenezcan a cada una de las clases por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_naive_bayes_information(_X_train, _Y_train):\n",
    "    naive_bayes_classifier = GaussianNB().fit(_X_train, _Y_train)\n",
    "    \n",
    "    for i in naive_bayes_classifier.classes_:\n",
    "        print('P(Y={}) = {:.2f}'.format(i, naive_bayes_classifier.class_prior_[i]))\n",
    "    \n",
    "    _df_naive = pd.DataFrame(\n",
    "            data=\n",
    "            {\n",
    "                'clase_0_media': naive_bayes_classifier.theta_[0],\n",
    "                'clase_1_media': naive_bayes_classifier.theta_[1],\n",
    "                'clase_0_varianza': naive_bayes_classifier.sigma_[0],\n",
    "                'clase_1_varianza': naive_bayes_classifier.sigma_[1],\n",
    "            }\n",
    "    )\n",
    "    print(_df_naive.describe())\n",
    "    return _df_naive\n",
    "\n",
    "#plot_naive_bayes_information(X_train, Y_train)\n",
    "\n",
    "# see the features when the known label is a fixed class\n",
    "for _clase in np.unique(Y_train_np):\n",
    "    print('Descripción de los datos de entrenamiento cuando la clase es {}'.format(_clase))\n",
    "    print(X_train[Y_train_np==_clase].describe())\n",
    "\n",
    "_seems_not_normal_0 = test_normality_on(X_train_np[Y_train_np==0])\n",
    "_seems_not_normal_1 = test_normality_on(X_train_np[Y_train_np==1])\n",
    "\n",
    "print('Se puede rechazar que {} probabilidades condicionales P(X_i=x_i /Y=0) provengan de una \\\n",
    "distribución normal con un nivel de significancia de {}'.format(_seems_not_normal_1.sum(), alpha_value))\n",
    "\n",
    "print('Se puede rechazar que {} probabilidades condicionales P(X_i=x_i /Y=1) provengan de una \\\n",
    "distribución normal con un nivel de significancia de {}'.format(_seems_not_normal_0.sum(), alpha_value))\n",
    "\n",
    "print('De estos casos se intersecan {} predictores'.format(_seems_not_normal_0[_seems_not_normal_1].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos permite decir que no valían todas las asunciones hechas al utilizar el método. Sin embargo el valor promedio AUC ROC obtenido sobre los datos de validación fue mejor que al utilizar árboles de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grid_knn = knn_grid_search_parametrization(X_train_np, Y_train_np)\n",
    "top_results(grid_knn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Salteando 0 combinaciones\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shrinkage</th>\n",
       "      <th>solver</th>\n",
       "      <th>mean_score_validation</th>\n",
       "      <th>mean_score_training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.7</td>\n",
       "      <td>lsqr</td>\n",
       "      <td>0.8452</td>\n",
       "      <td>0.9445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.7</td>\n",
       "      <td>eigen</td>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.9445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.6</td>\n",
       "      <td>eigen</td>\n",
       "      <td>0.8431</td>\n",
       "      <td>0.9583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.6</td>\n",
       "      <td>lsqr</td>\n",
       "      <td>0.8429</td>\n",
       "      <td>0.9584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.8</td>\n",
       "      <td>lsqr</td>\n",
       "      <td>0.8415</td>\n",
       "      <td>0.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.8</td>\n",
       "      <td>eigen</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.9254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.5</td>\n",
       "      <td>eigen</td>\n",
       "      <td>0.8369</td>\n",
       "      <td>0.9688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.9</td>\n",
       "      <td>lsqr</td>\n",
       "      <td>0.8368</td>\n",
       "      <td>0.8939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.5</td>\n",
       "      <td>lsqr</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.9688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.9</td>\n",
       "      <td>eigen</td>\n",
       "      <td>0.8364</td>\n",
       "      <td>0.8945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shrinkage solver  mean_score_validation  mean_score_training\n",
       "15       0.7   lsqr                 0.8452               0.9445\n",
       "16       0.7  eigen                 0.8451               0.9445\n",
       "14       0.6  eigen                 0.8431               0.9583\n",
       "13       0.6   lsqr                 0.8429               0.9584\n",
       "17       0.8   lsqr                 0.8415               0.9250\n",
       "18       0.8  eigen                 0.8412               0.9254\n",
       "12       0.5  eigen                 0.8369               0.9688\n",
       "19       0.9   lsqr                 0.8368               0.8939\n",
       "11       0.5   lsqr                 0.8367               0.9688\n",
       "20       0.9  eigen                 0.8364               0.8945"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GridSearch\n",
    "def linear_discriminant_analysis_grid_search_parametrization(_X_train, _Y_train):\n",
    "    lda_arguments = [\n",
    "        {'solver': ['svd']},\n",
    "        {\n",
    "            'solver': ['lsqr', 'eigen'],\n",
    "            'shrinkage': list(np.linspace(0,1,11)) + [None,'auto']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    lda_classifier = LinearDiscriminantAnalysis()\n",
    "    \n",
    "    return apply_grid_search_on_classifier_and_parameters(lda_classifier, lda_arguments, _X_train, _Y_train)\n",
    "    \n",
    "#grid_lda = linear_discriminant_analysis_grid_search_parametrization(X_train_np, Y_train_np)\n",
    "top_results(grid_lda, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros\n",
    "Se eligió variar el solver utilizado y utilizar valores de shrinkage cuando el solver no es SVD. Los demás parámetros se ignoran dado que no modifican la performance del clasificador.\n",
    "\n",
    "### Resultados\n",
    "Este método hace una asunción:\n",
    "    + La probabilidad condicional de observar un nuevo dato x dado que pertenece a la clase k, es decir P(X=x / Y=k) tiene una distribución gaussiana con una misma matriz de covarianza.\n",
    "\n",
    "\n",
    "Los resultados obtenidos en sus mejores combinaciones lo posicionan mejor que $\\textbf{KNN}$ mostrándolo como una alternativa.\n",
    "\n",
    "\n",
    "Es de interés probar que sucedería si intentamos reducir la dimensión de los datos utilizando LDA y luego volver a experimentar con los mismo hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def are_different_between_cov_matrix(_X_train, _Y_train, alpha=None):\n",
    "    \"\"\"\n",
    "    Assume X_train_np[Y_train_np==1] = alpha * X_train_np[Y_train_np==0]\n",
    "    with alpha a real number\n",
    "    \n",
    "    Compute alpha with two matrix norms or used it to compare\n",
    "    \"\"\"\n",
    "    cov_1 = np.cov(X_train_np[Y_train_np==1])\n",
    "    cov_0 = np.cov(X_train_np[Y_train_np==0])\n",
    "    \n",
    "    if alpha is None:\n",
    "        alpha = np.linalg.norm(cov_1) / np.linalg.norm(cov_0)\n",
    "    \n",
    "    return True if alpha == np.linalg.norm(cov_1,1) / np.linalg.norm(cov_0,1) else False\n",
    "\n",
    "if not are_different_between_cov_matrix(X_train_np, Y_train_np):\n",
    "    print('Las probabilidades condicionales no comparten la misma matriz de covarianza.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def contruct_lda_classifier_from_parameters(lda_paramaters, _X_train, _Y_train, n_components=None):\n",
    "    solver = lda_paramaters['solver']\n",
    "    shrinkage = lda_paramaters['shrinkage']\n",
    "    \n",
    "    return LinearDiscriminantAnalysis(\n",
    "            solver=solver,\n",
    "            shrinkage= None if shrinkage is None or np.isnan(shrinkage) else float(shrinkage),\n",
    "            n_components=n_components\n",
    "    ).fit(_X_train, _Y_train)\n",
    "\n",
    "def build_data_frame_from_grid_with_top(grid_lda, top=10, skip=0):    \n",
    "    lda_df = pd.DataFrame(grid_lda.cv_results_['params'])\n",
    "    lda_df[\"mean_score_validation\"] = grid_lda.cv_results_[\"mean_test_score\"]\n",
    "    lda_df[\"mean_score_training\"] = grid_lda.cv_results_[\"mean_train_score\"]\n",
    "    return lda_df.sort_values(by=\"mean_score_validation\", ascending=False).head(top+skip).tail(top)\n",
    "\n",
    "def get_transformed_input(_grid_lda, _X_train, _Y_train):\n",
    "    _lda_X_transformed = None\n",
    "    for index, parameter_comnbination in build_data_frame_from_grid_with_top(_grid_lda, 20).iterrows():\n",
    "        if parameter_comnbination['solver'] == 'lsqr':\n",
    "            continue\n",
    "        _lda_classifier = contruct_lda_classifier_from_parameters(\n",
    "            parameter_comnbination, _X_train, _Y_train, n_components=1\n",
    "        )\n",
    "        _lda_classifier.fit(_X_train, _Y_train)\n",
    "        _lda_X_transformed = _lda_classifier.transform(_X_train)\n",
    "        break\n",
    "    \n",
    "    print('Número original de predictores:', _X_train.shape)\n",
    "    print('Número reducido de predictores:', _lda_X_transformed.shape)\n",
    "\n",
    "    return _lda_X_transformed\n",
    "\n",
    "def transform_X_train_and_re_run_grid(grid_lda, _X_train, _Y_train):\n",
    "    _X_train_lda = get_transformed_input(grid_lda, X_train_np, Y_train_np)\n",
    "    return _X_train_lda, linear_discriminant_analysis_grid_search_parametrization(_X_train_lda, _Y_train)\n",
    "\n",
    "\n",
    "X_train_transformed, _grid_lda_with_transformed = transform_X_train_and_re_run_grid(\n",
    "    grid_lda, X_train_np, Y_train_np\n",
    ")\n",
    "\n",
    "print('\\nCorriendo LDA con los datos de entrenamiento transformados\\n')\n",
    "\n",
    "top_results(_grid_lda_with_transformed, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que haber transformado datos de entrada, mejora considerablemente ambos scores. Lo cual también puede significar overfitting sobre los datos. Se decidió utilizar estos datos transformados con dimensión reducida como entrenamiento de otro algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de los datos después de la reducción de tamaño (450, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfVHTFZwF5wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2277aa6470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Dimensión de los datos después de la reducción de tamaño {}'.format(X_train_transformed.shape))\n",
    "X_train_transformed.reshape(1,450)\n",
    "\n",
    "_df_transformed = pd.DataFrame(data={'class': Y_train_np})\n",
    "_df_transformed['feature'] = X_train_transformed\n",
    "\n",
    "print('Distribución de las clases después de reducir la dimensión')\n",
    "sns.distplot(_df_transformed[_df_transformed['class'] == 0]['feature'], label='class 0')\n",
    "sns.distplot(_df_transformed[_df_transformed['class'] == 1]['feature'], label='class 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Salteando 0 combinaciones\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>dual</th>\n",
       "      <th>fit_intercept</th>\n",
       "      <th>loss</th>\n",
       "      <th>penalty</th>\n",
       "      <th>mean_score_validation</th>\n",
       "      <th>mean_score_training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0000e-03</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9343</td>\n",
       "      <td>0.9348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0000e+02</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.9343</td>\n",
       "      <td>0.9348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0000e-05</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.9343</td>\n",
       "      <td>0.9348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.0000e-05</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.9343</td>\n",
       "      <td>0.9348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.0000e-04</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.9343</td>\n",
       "      <td>0.9348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             C   dual  fit_intercept           loss penalty  \\\n",
       "0   1.0000e-03  False          False  squared_hinge      l1   \n",
       "23  1.0000e+02  False           True  squared_hinge      l2   \n",
       "25  5.0000e-05  False          False  squared_hinge      l2   \n",
       "27  5.0000e-05  False           True  squared_hinge      l2   \n",
       "29  5.0000e-04  False          False  squared_hinge      l2   \n",
       "\n",
       "    mean_score_validation  mean_score_training  \n",
       "0                  0.9343               0.9348  \n",
       "23                 0.9343               0.9348  \n",
       "25                 0.9343               0.9348  \n",
       "27                 0.9343               0.9348  \n",
       "29                 0.9343               0.9348  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Salteando 0 combinaciones\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "      <th>splitter</th>\n",
       "      <th>mean_score_validation</th>\n",
       "      <th>mean_score_training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gini</td>\n",
       "      <td>3.0</td>\n",
       "      <td>None</td>\n",
       "      <td>210.0</td>\n",
       "      <td>best</td>\n",
       "      <td>0.9166</td>\n",
       "      <td>0.9536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gini</td>\n",
       "      <td>3.0</td>\n",
       "      <td>None</td>\n",
       "      <td>160.0</td>\n",
       "      <td>best</td>\n",
       "      <td>0.9166</td>\n",
       "      <td>0.9536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gini</td>\n",
       "      <td>3.0</td>\n",
       "      <td>None</td>\n",
       "      <td>360.0</td>\n",
       "      <td>best</td>\n",
       "      <td>0.9166</td>\n",
       "      <td>0.9536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gini</td>\n",
       "      <td>3.0</td>\n",
       "      <td>None</td>\n",
       "      <td>460.0</td>\n",
       "      <td>best</td>\n",
       "      <td>0.9166</td>\n",
       "      <td>0.9536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gini</td>\n",
       "      <td>3.0</td>\n",
       "      <td>None</td>\n",
       "      <td>310.0</td>\n",
       "      <td>best</td>\n",
       "      <td>0.9166</td>\n",
       "      <td>0.9536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   criterion  max_depth max_features  max_leaf_nodes splitter  \\\n",
       "8       gini        3.0         None           210.0     best   \n",
       "6       gini        3.0         None           160.0     best   \n",
       "14      gini        3.0         None           360.0     best   \n",
       "18      gini        3.0         None           460.0     best   \n",
       "12      gini        3.0         None           310.0     best   \n",
       "\n",
       "    mean_score_validation  mean_score_training  \n",
       "8                  0.9166               0.9536  \n",
       "6                  0.9166               0.9536  \n",
       "14                 0.9166               0.9536  \n",
       "18                 0.9166               0.9536  \n",
       "12                 0.9166               0.9536  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Salteando 0 combinaciones\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_score_validation</th>\n",
       "      <th>mean_score_training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9343</td>\n",
       "      <td>0.9348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_score_validation  mean_score_training\n",
       "0                 0.9343               0.9348"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_knn_transformed = support_vector_machine_grid_serch_parametrization(X_train_transformed, Y_train_np)\n",
    "top_results(grid_knn_transformed)\n",
    "\n",
    "grid_tree_transformed = tree_grid_search_parametrization(X_train_transformed, Y_train_np)\n",
    "top_results(grid_tree_transformed)\n",
    "\n",
    "grid_naive_transformed = naive_bayes_grid_search_parametrization(X_train_transformed, Y_train_np)\n",
    "top_results(grid_naive_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Salteando 0 combinaciones\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>dual</th>\n",
       "      <th>fit_intercept</th>\n",
       "      <th>loss</th>\n",
       "      <th>penalty</th>\n",
       "      <th>mean_score_validation</th>\n",
       "      <th>mean_score_training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0500</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8502</td>\n",
       "      <td>0.9687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0500</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8494</td>\n",
       "      <td>0.9688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.8444</td>\n",
       "      <td>0.9568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.8444</td>\n",
       "      <td>0.9572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.8438</td>\n",
       "      <td>0.9354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         C   dual  fit_intercept           loss penalty  \\\n",
       "36  0.0500  False          False  squared_hinge      l1   \n",
       "38  0.0500  False           True  squared_hinge      l1   \n",
       "1   0.0010  False          False  squared_hinge      l2   \n",
       "3   0.0010  False           True  squared_hinge      l2   \n",
       "31  0.0005  False           True  squared_hinge      l2   \n",
       "\n",
       "    mean_score_validation  mean_score_training  \n",
       "36                 0.8502               0.9687  \n",
       "38                 0.8494               0.9688  \n",
       "1                  0.8444               0.9568  \n",
       "3                  0.8444               0.9572  \n",
       "31                 0.8438               0.9354  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GridSearch\n",
    "def support_vector_machine_grid_serch_parametrization(_X_train, _Y_train):\n",
    "    C_range = [10**e for e in range(-3, 3)] + [0.05 * 10**e for e in range(-3, 3)]\n",
    "    SVM_Linear_arguments = [\n",
    "        {\n",
    "            'C': C_range,\n",
    "            'loss': ['squared_hinge'],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'dual': [False],\n",
    "            'fit_intercept': [False, True]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    linearsvm_classifier = sklearn.svm.LinearSVC()\n",
    "\n",
    "    return apply_grid_search_on_classifier_and_parameters(\n",
    "        linearsvm_classifier, SVM_Linear_arguments, _X_train, _Y_train\n",
    "    )\n",
    "\n",
    "grid_svm_linear = support_vector_machine_grid_serch_parametrization(X_train_np, Y_train_np)\n",
    "top_results(grid_svm_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros:\n",
    "\n",
    "Se eligió ese rango específico de valores de C para permitirle a SVM 'ignorar' muestras de entrenamiento cerca del margen y permitirse colocarlas en el lado equivocado (lo que ocurre con valores C chicos). Esto sirve especialmente en los casos donde hay mucho 'ruido' presente en la muestra.\n",
    "\n",
    "Por otro lado se recomienda utilizar valores de C grandes cuando la penalidad es $\\textit{l1}$ para que más predictores sean considerados (http://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use)\n",
    "\n",
    "### Resultados:\n",
    "\n",
    "El utilizar un C chico, le permitió superar a los clasificadores previos. Esto nos hace dudar de la existencia de ruido en los datos de entrenamiento (sumado a lo ya planteado de no ser linealmente separable).\n",
    "\n",
    "\n",
    "### Observaciones:\n",
    "SVM necesita que los datos esten en la misma escala. Para la corrida, ig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generic SVM\n",
    "def generic_support_vector_machine_grid_search_parametrization(_X_train, _Y_train):\n",
    "    C_range = [0.01 * 10**e for e in range(6)] + [0.05 * 10**e for e in range(6)]\n",
    "    SVM_General_arguments = [\n",
    "        {\n",
    "            'kernel': ['rbf'],\n",
    "            'C': C_range,\n",
    "            'gamma': ['auto'],#list(np.linspace(0.0001, 1, 10)),\n",
    "            'shrinking': [True, False]\n",
    "        },\n",
    "        {\n",
    "            'kernel': ['sigmoid'],\n",
    "            'coef0': np.linspace(0,10,11),\n",
    "            'C': C_range,\n",
    "            'gamma': ['auto'],#list(np.linspace(0.0001, 1, 10)),\n",
    "            'shrinking': [True, False]\n",
    "        },\n",
    "        {\n",
    "            'kernel': ['poly'],\n",
    "            'coef0': np.linspace(0,10,11),\n",
    "            'gamma': ['auto'],#list(np.linspace(0.0001, 1, 10)),\n",
    "            'degree': np.linspace(2,10,9),\n",
    "            'C': C_range,\n",
    "            'shrinking': [True, False]\n",
    "        }\n",
    "    ]\n",
    "    \"\"\"\n",
    "        {\n",
    "            'kernel': ['precomputed'],\n",
    "            'C': np.linspace(0.01,1000,11),\n",
    "            'shrinking': [True, False]\n",
    "        },\n",
    "    \"\"\"\n",
    "\n",
    "    general_svm_classifier = sklearn.svm.SVC()\n",
    "    \n",
    "    return apply_grid_search_on_classifier_and_parameters(\n",
    "        general_svm_classifier, SVM_General_arguments, _X_train, _Y_train\n",
    "    )\n",
    "\n",
    "grid_svm_general = generic_support_vector_machine_grid_search_parametrization(X_train_np, Y_train_np)\n",
    "top_results(grid_svm_general, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones Grid Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Conclusiones Grid Search\n",
    "\n",
    "def print_top(dicc, n):\n",
    "    values = [(v,k,) for k,v in dicc.items()]\n",
    "    print(sorted(values, key=lambda x: 1/x[0])[:10])\n",
    "    \n",
    "print_top(resultados_training, 10)\n",
    "print_top(resultados_validation, 10)\n",
    "\n",
    "#< < COMPLETAR > >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4: \n",
    "### Diagnóstico Sesgo-Varianza. \n",
    "\n",
    "En este punto, se pide inspeccionar dos de sus mejores modelos encontrados hasta ahora: el mejor modelo de tipo árbol de decisión y el mejor de tipo SVM. Para ello:\n",
    "\n",
    "1. Graficar curvas de complejidad para cada modelo, variando la profundidad en el caso de árboles, y el hiperparámetro C en el caso de SVM. Diagnosticar cómo afectan al sesgo y a la varianza esos dos hiperparámetros.\n",
    "2. Graficar curvas de aprendizaje para cada modelo. En base a estas curvas, sacar conclusiones sobre si los algoritmos parecen haber alcanzado su límite, o bien si aumentar la cantidad de datos debería ayudar.\n",
    "3. Construir un modelo RandomForest con 200 árboles. Explorar para qué sirve el hiperparámetro max_features y cómo afecta a la performance del algoritmo mediante una curva de complejidad. Explicar por qué creen que se dieron los resultados obtenidos. Por último, graficar una curva de aprendizaje sobre los parámetros elegidos para determinar si sería útil o no conseguir más datos (usar  grid search para encontrar una buena combinación de parámetros).  \n",
    "\n",
    "\n",
    "**Atención**: Tener en cuenta que debemos seguir utilizando ROC AUC como métrica para estas curvas.\n",
    "\n",
    "**ver**: http://scikit-learn.org/stable/modules/learning_curve.html#learning-curve\n",
    "\n",
    "----\n",
    "**EJERCICIO EXTRA:** Utilizar RandomizedSearchCV para explorar la performance del algoritmo de Gradient Boosting y comparar con los resultados obtenidos en el punto (c).\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competencias\n",
    "\n",
    "La entrega del trabajo estará acompañada de una competencia en la cual deberán poner a prueba su mejor modelo y sobre todo, su capacidad para estimar sus resultados. \n",
    "\n",
    "Su tarea será estimar la performance (AUC ROC) que tendrá su mejor modelo en datos de evaluación (X_competencia). \n",
    "\n",
    "Para ello, deberán predecir las probabilidades de las distintas instancias con su modelo, enviarnos dichas probabilidades junto a una estimación con 4 decimales de cuál será el AUC ROC resultante y calcularemos el resultado real. El grupo que consiga acercarse más al valor real, será el grupo ganador.  \n",
    "\n",
    "Recomendamos no perder de vista esta competencia en el momento de separar los datos en los primeros puntos. \n",
    "\n",
    "Para esto, junto con la entrega del informe, deberán enviar un archivo en formato csv con las columnas “index” y “output” (ver ejemplo de archivo en: [y_competencia_ejemplo.csv](https://github.com/pbrusco/aa-notebooks/blob/master/TP1/y_competencia_ejemplo.csv)) y un valor esperado de AUC ROC. \n",
    "\n",
    "\n",
    "## Entrega\n",
    "- Contarán con un esqueleto en formato Jupyter Notebook en donde tendrán que completar las celdas faltantes (ya sea con explicaciones y gráficos o código). \n",
    "- El notebook final deberá ser entregado en formatos .html e .ipynb. Es necesario que los resultados puedan reproducirse al ejecutar todas las celdas en orden (Kernel - Restart and Run All) utilizando las bibliotecas requeridas en el archivo: requirements.txt del repositorio. \n",
    "- Tienen tiempo hasta las 23:59hs del día miércoles 17/10/2018. La entrega se debe realizar a través del campus virtual y debe contener el informe.\n",
    "- El trabajo deberá elaborarse en grupos de 3 personas.\n",
    "- Se podrán pedir pruebas de integridad y autoría; es decir, verificar que la salida solicitada es fruto del modelo presentado y que el modelo fue construido según lo requerido en este enunciado.\n",
    "- La evaluación será grupal y se basará en la calidad del informe (presentación, claridad, prolijidad); la originalidad, practicidad y coherencia técnica de la solución; la corrección y solidez de las pruebas realizadas.\n",
    "- En el primer parcial se incluirá una pregunta sobre la solución entregada. Esa pregunta no influirá en la nota del parcial, pero sí en la nota individual del TP1.\n",
    "- La participación en la competencia es obligatoria. De todas maneras, el resultado no incidirán en la nota de la materia.\n",
    "- Los ejercicios extra son opcionales para aprobar el TP, pero son obligatorios para promocionar la materia.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
